{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeonsub/models_from_scratch/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DbAWfKIxnu5J"
      },
      "outputs": [],
      "source": [
        "# my github: https://github.com/withAnewWorld/models_from_scratch\n",
        "# my blog\n",
        "# https://self-deeplearning.blogspot.com/\n",
        "# https://self-deeplearning.tistory.com/\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT10fPvLXs0w"
      },
      "source": [
        "## Ref\n",
        "1. Transformer <br>\n",
        "paper(Attention Is All You Need): https://arxiv.org/abs/1706.03762 <br>\n",
        "cs231n transformer slide: http://cs231n.stanford.edu/slides/2021/lecture_11.pdf<br>\n",
        "blog: http://peterbloem.nl/blog/transformers <br>\n",
        "github: https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb<br>\n",
        "official PyTorch Transformer code: https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer <br>\n",
        "\n",
        "2. Positional Encoding <br>\n",
        "blog: https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3 <br>\n",
        "blog: https://hongl.tistory.com/231 <br>\n",
        "blog: https://velog.io/@gibonki77/DLmathPE <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbKoRSIjZGxS"
      },
      "source": [
        "## 목차\n",
        "1. Transformer (roughly) <br>\n",
        "  - Embedding, Skip Connection, Feed Forward Network\n",
        "  - Data Flow in Transformer\n",
        "2. Multi-Head Attention<br>\n",
        "  - Attention\n",
        "  - Self Attention\n",
        "  - Multi-Head Attention\n",
        "  - Vectorized Multi-Head Attention\n",
        "3. Masked Multi-Head Attention <br>\n",
        "4. Transformer (without Positional Encoding) <br>\n",
        "5. Positional Encoding <br>\n",
        "  - Count\n",
        "  - Normalized Count\n",
        "  - Binary indexing\n",
        "  - Sinusoidal Positional Encoding\n",
        "6. Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1prqpTlkZ8iS"
      },
      "source": [
        "## Transformer (roughly)\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/transformer.PNG\" height = 500>\n",
        "</p>\n",
        "\n",
        "\n",
        "Transformer는 기존 RNN기반 Seq2Seq와 비슷하게 Encoder(왼쪽 모듈)와 Decoder(오른쪽 모듈)로 이루어져 있지만, <br>\n",
        "안의 내용은 완전히 탈바꿈한 Deep Learning Model 입니다. <br>\n",
        "\n",
        "먼저, Embedding과 Add & Norm 그리고 Feed Forward 블럭(block)은 아실법한 내용이니 간단히 설명드리겠습니다. <br>\n",
        "\n",
        "1. Embedding <br>\n",
        "Embedding은 lookup table을 이용하여 sequential data를 벡터로 변환하는 기법입니다. <br>\n",
        "(PyTorch는 nn.Embedding을 통해 해당 기법을 제공하고 있습니다.)<br>\n",
        "\n",
        "2. Add & Norm <br>\n",
        "Add의 경우 ResNet에서 개발된 residual connection(or skip connection)을 의미합니다. <br>\n",
        "Norm의 경우 Batch Normalization 또는 Layer Normalization을 의미합니다. <br> Transformer에서는 Layer Normalization을 사용했습니다. <br>\n",
        "즉 해당 block은 다음과 같습니다. <br>\n",
        "\n",
        "#### <center> LayerNorm($x$ + Sublayer($x$)) </center>\n",
        "\n",
        "3. Feed Forward <br>\n",
        "해당 block은 Linear Layer와 활성함수(activation func)를 묶어놓은 <br>\n",
        "다중 퍼셉트론 레이어(Multi Layer Perceptron)입니다. <br>\n",
        "\n",
        "#### <center> FFN($x$) = $max(0, xW_{1} + b_{1})W_{2} + b_{2}$ </center>\n",
        "\n",
        "## Data Flow in Transformer\n",
        "설명을 위해 Transformer를 이용하여 번역(translation) task를 수행하는 경우로 가정하겠습니다.\n",
        "\n",
        "1. Transformer Encoder\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/transformer_encoder.PNG\" height = 500>\n",
        "</p>\n",
        "\n",
        "RNN이 옆으로 Layer를 쌓았다면 Transformer는 Stack과 같이 위로 Layer를 쌓는 구조입니다(CNN 기반 모델과 비슷하게). <br>\n",
        "\n",
        "즉 Encoder Block 옆의 (Nx)은 얼마나 Layer를 쌓을 것인지 나타내는 기호입니다. <br>\n",
        "\n",
        "생소하신 부분은 Multi-Head Attention 모듈 진입 전 데이터가 세 부분으로 나뉘는 구조일 것입니다. <br>\n",
        "해당 구조는 이전 연산의 결과값이 나눠지는 것(split)이 아니라 각각 온전한 결과값을 받아 특정 연산을 처리한다고 우선 이해하시면 됩니다. <br>\n",
        "\n",
        "```python\n",
        "# pseudo code\n",
        "class TransformerEncoder(nn.Module):\n",
        "  ...\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    inputs:\n",
        "      x(Tensor[batch_size, source_length])\n",
        "    returns:\n",
        "      encoder_out(Tensor[batch_size, source_length, embed_dim])\n",
        "    '''\n",
        "    # 약어 정의 (src_len: source_length, emb_dim: embed_dim, enc: encoder, mlp:Multi Layer Perceptron)\n",
        "\n",
        "    embedded = Embedding(x) # (batch_size, src_len) -> (batch_size, src_len, emb_dim)\n",
        "    encoder_input = positional_encoding(embedded) + embedded # (batch_size, src_len, emb_dim)\n",
        "    \n",
        "    # 반복 시작(for N)\n",
        "    attended = multi_head_attention(encoder_input, encoder_input, encoder_input) #(batch_size, src_len, emb_dim)\n",
        "    skip_connection = encoder_input + attended\n",
        "    normalized = LayerNorm(skip_connection)\n",
        "\n",
        "    mlp_out = feed_forward(normalized)\n",
        "    # mlp_out.size(): (batch_size, src_len, emb_dim)\n",
        "\n",
        "    skip_connection = mlp_out + normalized\n",
        "    normalized = LayerNorm(skip_connection)\n",
        "\n",
        "    encoder_input = normalized # (batch_size, src_len, emb_dim)\n",
        "    # 반복 끝(End for)\n",
        "    \n",
        "    enc_out = encoder_input\n",
        "    return enc_out\n",
        "\n",
        "```\n",
        "\n",
        "Encoder 안에서 데이터가 어떻게 흐르는지 대충 윤곽이 잡히실 것 같으니 Decoder로 넘어가겠습니다. <br>\n",
        "\n",
        "2. Transformer Decoder\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/transformer_decoder.PNG\" height = 500>\n",
        "</p>\n",
        "\n",
        "기존의 Seq2Seq 모델과 같이 Transformer Decoder는 Encoder의 결과값을 받아 연산을 수행합니다. <br>\n",
        "\n",
        "```python\n",
        "# pseudo code\n",
        "class TransformerDecoder(nn.Module):\n",
        "  ...\n",
        "  def forward(self, y, enc_out):\n",
        "    '''\n",
        "    inputs:\n",
        "      y(Tensor[batch_size, target_length])\n",
        "      enc_out(Tensor[batch_size, source_length, embed_dim])\n",
        "    returns:\n",
        "      output_probabilities(Tensor[batch_size, target_length, output_dim)\n",
        "    '''\n",
        "    # 약어 정의 (tgt_len: target_length, emb_dim: embed_dim, dec: decoder, mlp: Multi Layer Perceptron)\n",
        "\n",
        "    embedded = Embedding(y) # (batch_size, tgt_len) -> (batch_size, tgt_len, emb_dim)\n",
        "    dec_input = positional_encoding(embedded) + embedded # (batch_size, tgt_len, emb_dim)\n",
        "    \n",
        "    # 반복 시작(for N)\n",
        "    masked_attended = masked_multi_head_attention(dec_input, dec_input, dec_input) #(batch_size, tgt_len, emb_dim)\n",
        "    skip_connection = dec_input + masked_attended\n",
        "    normalized = LayerNorm(skip_connection)\n",
        "\n",
        "    attended = multi_head_attention(enc_out, enc_out, normalized)\n",
        "    # enc_out: (batch_size, src_len, emb_dim)\n",
        "    # normalized: (batch_size, tgt_len, emb_dim)\n",
        "    # attended: (batch_size, tgt_len, emb_dim)\n",
        "\n",
        "    skip_connection = normalized + attended\n",
        "    normalized = LayerNorm(skip_connection)\n",
        "\n",
        "    mlp_out = feed_forward(normalized)\n",
        "    # mlp_out.size(): (batch_size, tgt_len, emb_dim)\n",
        "\n",
        "    skip_connection = mlp_out + normalized\n",
        "    normalized = LayerNorm(skip_connection)\n",
        "\n",
        "    dec_input = normalized # (batch_size, tgt_len, emb_dim)\n",
        "    # 반복 끝(End for)\n",
        "    output_probabilities = Softmax(Linear(dec_input)) # (batch_size, tgt_len, out_dim)\n",
        "    return output_probabilities\n",
        "\n",
        "```\n",
        "\n",
        "cf) Encoder의 N(layer 개수)와 Decoder의 N(layer의 개수)는 서로 같지 않아도 됩니다. (표기상의 편리성때문에 모두 N으로 표시한 것) <br>\n",
        "\n",
        "이제 (Masked)Multi-Head Attention과 <br>\n",
        "Positional Encoding만 무엇인지 알면 <br>\n",
        "Transformer에 대한 이해는 끝나게 됩니다. <br>\n",
        "우선 알고있는 것을 빠르게 구현한 후 <br>\n",
        "(Masked)Multi-Head Attention과 Positional Encoding에 대해 설명 드리겠습니다.<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JqoepZBnRNe"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(seq_length, emb_dim)\n",
        "    self.positional_encoding = PositionalEncoding()\n",
        "\n",
        "    self.encoder_layer = nn.ModuleList(\n",
        "        [EncoderBlock() for _ in range(n_layer)]\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    transformer_input = self.positional_encoding(x) + self.embed(x)\n",
        "    for encoder_block in self.encoder_layer:\n",
        "      transformer_input = encoder_block(transformer_input)\n",
        "\n",
        "    out = transformer_input\n",
        "    return out\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.multi_head_attn = MultiHeadAttn()\n",
        "    self.norm1 = nn.LayerNorm()\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(emb_dim, 4*emb_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*emb_dim, emb_dim)\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm()\n",
        "\n",
        "  def forward(self, x):\n",
        "    attended = self.norm1(self.multi_head_attn(x) + x)\n",
        "    return self.norm2(self.feed_forward(attended) + attended)\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(seq_length, emb_dim)\n",
        "    self.positional_encoding = PositionalEncoding()\n",
        "\n",
        "    self.decoder_layer = nn.ModuleList(\n",
        "        [DecoderBlock() for _ in range(n_layer)]\n",
        "        )\n",
        "\n",
        "    self.fc_out = nn.Sequential(\n",
        "        nn.Linear(emb_dim, out_dim),\n",
        "        nn.SoftMax()\n",
        "    )\n",
        "\n",
        "  def forward(self, x, enc_out):\n",
        "    decoder_input = self.positional_encoding(x) + self.embed(x)\n",
        "    for decoder_block in self.decoder_layer:\n",
        "      decoder_input = decoder_block(decoder_input, enc_out, enc_out)\n",
        "\n",
        "    out = decoder_input\n",
        "    return self.fc_out(out)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.masked_multi_head_attn = MaskedMultiHeadAttn()\n",
        "    self.norm1 = nn.LayerNorm()\n",
        "\n",
        "    self.multi_head_attn = MultiHeadAttn()\n",
        "    self.norm2 = nn.LayerNorm()\n",
        "\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(emb_dim, 4*emb_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*emb_dim, emb_dim)\n",
        "    )\n",
        "    self.norm3 = nn.LayerNorm()\n",
        "\n",
        "  def forward(self, x, enc_query, enc_key):\n",
        "    masked_attended= self.norm1(self.masked_multi_head_attn(x) + x)\n",
        "    attended = nn.norm2(self.multi_head_attn(enc_query, enc_key, masked_attended) + masked_attended)\n",
        "    return self.norm3(self.feed_forward(attended) + attended)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  pass\n",
        "\n",
        "class MultiHeadAttn(nn.Module):\n",
        "  pass\n",
        "\n",
        "class MaskedMultiHeadAttn(nn.Module):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1dqVbpLLf8s"
      },
      "source": [
        "## Multi Head Attention\n",
        "\n",
        "1. Attention\n",
        "\n",
        "Attention은 한국어로 집중, 관심사항 정도로 번역할 수 있습니다. <br>\n",
        "Deep Learning에서 Attention은 어느 데이터에 '집중'할 것인가? 정도로 간략히 해석할 수 있습니다. <br>\n",
        "우선, 예시를 위해 영어 문장을 한국어 문장으로 번역하는 경우에 대해 살펴보겠습니다. <br>\n",
        "\n",
        "Source Sentence: \"Attention Is All You Need\" <br>\n",
        "\n",
        "머리 속으로든 사전을 이용하든 다음과 같은 방식으로 번역이 진행될 것입니다. <br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/구글_검색.PNG\" height = 100, width = 700>\n",
        "</p>\n",
        "\n",
        "(google 검색엔진의 결과값으로는 [주목, 주의, 수리, 차려 자세, ...] 가 산출되네요.) <br>\n",
        "\n",
        "이 때, 중요한 점은 위의 예를 통해 알 수 있듯 Attention과 matching되는 한국어는 꾀나 많다는 점입니다. <br>\n",
        "이는 단어의 서순, 문장, 문맥, 문화 차이 등 다양한 변수에 의해 같은 단어(source word)도 다른 단어(target word)로 풀이되기 때문입니다. <br>\n",
        "\n",
        "따라서 해당 문장에서 \"Attention\"과 매칭되는 여러 한국어 중 <br>\n",
        "어떤 단어가 가장 적합한지 판단한 후 사용해야 번역이 올바르게 이루어집니다. <br>\n",
        "(Source 단어와 Target 단어간의 비교)\n",
        "\n",
        "이러한 내용을 알고리즘으로 구현한 것이 Attention mechanism 입니다. <br>\n",
        "<br>\n",
        "**Cosine Similarity** <br>\n",
        "\n",
        "NLP에서 단어는 Vector로 표현됩니다. <br>\n",
        "즉 두 단어를 비교하기 위해선 두 Vector를 비교해야합니다. <br>\n",
        "Vector를 비교하는 가장 손쉬운 방법 중 하나로는 Cosine Similarity(코사인 유사도)가 있습니다.<br>\n",
        "\n",
        "$$Cosine\\ Similarity(\\vec A, \\vec B) =  \\vec A \\cdot \\vec B/ (|\\vec A||\\vec B|)$$\n",
        "\n",
        "(2차원인 경우  $\\vec A \\cdot \\vec B = |\\vec A| |\\vec B| cos\\theta$) <br>\n",
        "해당 아이디어를 이용하여 Attention mechanism은 벡터의 내적을 이용하여 두 단어간의 유사도를 측정합니다. <br>\n",
        "cf) 사실 Attention mechanism에는 벡터의 내적말고도 다양한 방법론이 존재합니다. <br>\n",
        "더 자세히 알아보고 싶으시면 (Luong et al. Effective Approaches to Attention-based Neural Machine Translation. 2015) 을 참고해주세요<br>\n",
        "2. Self Attention <br>\n",
        "\n",
        "Self Attention은 위의 예에서 나온 Source 단어와 Target 단어간의 유사도를 측정하는 것이 아닌 <br>\n",
        "Source 문장 안에서 Source 단어끼리의 유사도를 측정하는 알고리즘입니다. (자기 참조) <br>\n",
        "\n",
        "cf) Self Attention은 Target sentence에 대해서도 수행할 수 있습니다. <br>\n",
        "<br>\n",
        "\n",
        "즉, sentence = ['attention', 'is', 'all', 'you', 'need'] 일 때, <br>\n",
        "\n",
        "Self Attention은 'attention' 단어와 자신을 포함한 나머지 단어(attention, is, all, you, need)간의 비교입니다. <br>\n",
        "그러면 Self Attention은 무슨 의미를 가지는 것인지 의문이 드실 것입니다. <br>\n",
        "<br>\n",
        "우선 일반적으로 Seq2Seq 모델은 모두 Encoder를 통해 Source Setence의 정보를 맥락 정보(Context Vector)로 압축하여 Decoder에 전달합니다. <br>\n",
        "즉 번역을 하기 위해서는 Source Sentence의 정보를 효율적으로 압축해야 합니다. <br>\n",
        "<br>\n",
        "사람이 번역하는 경우 맥락 정보에는 다음과 같은 사항들이 압축될 것입니다. <br>\n",
        "1) 각 단어가 가지는 여러 의미 <br>\n",
        "2) 각 단어의 성분(주어, 목적어, ...) <br>\n",
        "3) 각 단어가 가지는 긍정적 or 부정적 이미지 <br>\n",
        "&nbsp;&nbsp;&nbsp; .<br>\n",
        "&nbsp;&nbsp;&nbsp; .<br>\n",
        "&nbsp;&nbsp;&nbsp; .<br>\n",
        "-> 문장의 뜻(맥락) <br>\n",
        "<br>\n",
        "\n",
        "기존에는 RNN의 Recurrence 구조를 통해 Context Vector를 만들어냈다면 <br>\n",
        "Transformer는 Attention만을 이용해서 Context Vector를 만들어낸다는 것이 차이점입니다. <br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/구글_검색_1.PNG\" height = 100 width = 700>\n",
        "</p>\n",
        "\n",
        "여기서 주목할 점은 Attention mechanism은 마치 검색엔진을 이용하는 것과 비슷한 구조를 띤다는 점입니다. <br>\n",
        "\n",
        "즉, 묻고자 하는 바(단어간의 유사도)가 있고 이에 따른 결과값을 산출하는 구조입니다. <br>\n",
        "이는 프로그래밍적으로 <br>\n",
        "묻고자 하는 바: **Query** <br>\n",
        "Query와 비교할 데이터: **Key** <br>\n",
        "Key값을 통해 접근할 수 있는 데이터: **Value** <br>\n",
        "로 나타낼 수 있습니다. <br>\n",
        "\n",
        "원활한 이해를 돕기 위해 다음의 예시를 들겠습니다. <br>\n",
        "\n",
        "Query = \"Attention 한국어 뜻\" <br>\n",
        "Key = [\"Attention 한국어로 번역\", \"Attention\", \"Attention 노래\", \"NewJeans\", ...] <br>\n",
        "Value = [각 Key값에 대응(mapping)되는 웹페이지(html)] <br>\n",
        "Weighted Sum: Value를 통해 가공된 웹페이지(html) <br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/self_attention.PNG\" height = 200 width = 850>\n",
        "</p>\n",
        "\n",
        "(Attention의 설명을 위한 예시일 뿐입니다. 정확한 검색엔진 작동방식은 다른 문헌을 참고해주세요) <br>\n",
        "<br>\n",
        "\n",
        "Self Attention의 수식은 다음과 같습니다. <br>\n",
        "\n",
        "$$ y_{i} = \\sum_{j} softmax(W_{i,\\ j})x_{j} \\\\ (W_{i,\\ j} = x_{i} \\cdot x_{j}^{T}) \\\\(i: 현재\\ token,\\ j: 모든\\ token)$$ <br>\n",
        "Slef Attention의 수식을 Query, Key, Value를 이용하여 표현하면 다음과 같습니다. <br>\n",
        "\n",
        "$$ \\sum softmax(Q \\cdot K^{T})V$$\n",
        "\n",
        "\n",
        "1) softmax? <br>\n",
        "내적을 통해 유사도를 구한 후 softmax를 하는 이유는 cosine 유사도와 같이 <br>\n",
        "정규화(normalized)된 값을 얻기 위함입니다. <br>\n",
        "왜냐하면 Deep Learning 모델은 크기가 큰 값을 지속적으로 feed할 경우 훈련(train)이 적절하게 되지 않기 때문입니다. <br>\n",
        "그러면 cosine 유사도 대신 왜 softmax를 사용할까요? <br>\n",
        "이에 대한 이유로는 deep learning에서 데이터를 해석할 때 <br>\n",
        "확률로 해석하고자 하는 경향성이 있기 때문으로 생각합니다. <br>\n",
        "\n",
        "즉, Attention은 Query와 Key의 유사도 확률에 대한 Value의 가중합(Weighted Sum)으로 해석할 수 있습니다. <br>\n",
        "<br>\n",
        "\n",
        "2) 가중합? <br>\n",
        "훈련(train)이 적절하게 이루어질 경우 Embedding vector는 다음과 같은 특성을 띱니다. <br>\n",
        "vector(king) - vector(man) + vector(woman) $\\simeq$ vector(queen) <br>\n",
        "(ref: Efficient Estimation of Word Representations in\n",
        "Vector Space) <br>\n",
        "즉, 임베딩 벡터간 연산이 사람이 이해할 수 있는 방법으로 바뀌게 됩니다. <br>\n",
        "\n",
        "<br>\n",
        "한편, 가중합은 다음과 같은 결과를 나타냅니다. <br>\n",
        "\n",
        "p(attention) + p'(is) + p''(all) + ... <br>\n",
        "\n",
        "즉, 가중합을 통해 output vector는 표현력이 더욱 풍부해지는 효과를 가집니다. <br>\n",
        "ex) output = 90%의 (attention) 의미 + 5%의 (is) 의미 + 2%의 (all) 의미 + ...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzmjfiZJrEY8",
        "outputId": "ec6c367f-3f00-443d-aa33-b4b615996f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "similarity_p\n",
            "tensor([[[9.9637e-01, 1.9821e-04, 3.4106e-03, 2.2651e-05],\n",
            "         [1.6019e-03, 4.9172e-01, 1.1873e-01, 3.8795e-01],\n",
            "         [5.8131e-03, 2.5040e-02, 8.6330e-01, 1.0585e-01],\n",
            "         [4.9540e-08, 1.0499e-04, 1.3582e-04, 9.9976e-01]]])\n",
            "--------------------\n",
            "weighted_sum\n",
            "tensor([[[-0.9495, -1.3336,  0.4078,  1.0864,  1.1539],\n",
            "         [ 1.3426,  0.2499, -0.8715, -0.5918, -0.8535],\n",
            "         [ 0.7035, -0.5405, -1.8074,  0.1934, -0.3063],\n",
            "         [ 2.3354, -0.5784, -0.6451, -1.1405, -2.0048]]])\n"
          ]
        }
      ],
      "source": [
        "# self attention\n",
        "batch_size, seq_length, emb_dim = 1, 4, 5\n",
        "\n",
        "x = torch.randn(batch_size, seq_length, emb_dim)\n",
        "query, key, value = x, x, x\n",
        "\n",
        "similarity = torch.bmm(query, key.transpose(1, 2)) # (batch_size, seq_length, seq_length)\n",
        "\n",
        "similarity_p = F.softmax(similarity, dim = -1) # (batch_size, seq_length, seq_length)\n",
        "weighted_sum = torch.bmm(similarity_p, value) # (batch_size, seq_length, emb_dim)\n",
        "\n",
        "print('similarity_p')\n",
        "print(similarity_p)\n",
        "print('-'*20)\n",
        "print('weighted_sum')\n",
        "print(weighted_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW1qECUd7SEn"
      },
      "source": [
        "3. Multi-Head Attention\n",
        "\n",
        "Multi-Head Attention은 Attention 연산을 한 번에 여러 번 수행하는 것입니다. <br>\n",
        "즉 한 번의 검색을 통해 원하는 웹페이지를 찾는 대신에 <br>\n",
        "여러 번의 검색을 통해 원하는 웹페이지를 찾겠다는 생각입니다. <br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/multi_head_attn.PNG\" height = 300>\n",
        "</p>\n",
        "\n",
        "$$MultiHead(Q, K, V) = Concat(head_{1},\\ head_{2}, ...,\\ head_{h})W^{O} \\\\\n",
        "(head_{i} = Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i}) \\\\\n",
        "(Attention(Q, K, V) = softmax(QK^{T}/\\sqrt{d_{k}})V)$$\n",
        "\n",
        "<br>\n",
        "$$h: Number\\ of\\ Multi\\ Head(i= 1\\ to\\ h) \\\\$$\n",
        "\n",
        "$$Q: Query,\\ W^{Q}_{i}: Linear\\ Layer_{i}\\ of\\ Query \\\\\n",
        "K: Key,\\ W^{K}_{i}: Linear\\ Layer_{i}\\ of\\ Key \\\\\n",
        "V: Value,\\ W^{V}_{i}: Linear\\ Layer_{i}\\ of\\ Value \\\\ $$\n",
        "$$W^{O} = Linear\\ Layer\\ of\\ Output\\ (hd_{v}, d_{model})$$\n",
        "<br>\n",
        "1) Linear Layer? <br>\n",
        "Query, Key, Value에 Linear Layer를 거친 후 Attention을 하는 이유는 <br>\n",
        "Learnable Parameter(Linear Layer)를 통해 <br>\n",
        "각각의 embed vector가 $Query,\\ Key,\\ Value$에 적합한 벡터로 변환되기를 기대하기 때문입니다. <br>\n",
        "\n",
        "이 때 Linear Layer의 크기는 ($d_{model},\\ d_{k}$)입니다. ($d_{k} = d_{v} = d_{model} / h$)<br>\n",
        "\n",
        "2) $Q \\cdot K^{T}/\\sqrt{d_{k}}$? <br>\n",
        "$\\sqrt{d_{k}}$로 나누는 이유는 softmax는 값이 커질경우 <br>\n",
        "gradient exploding / vanishing 문제를 야기하기 때문에 해당 값으로 나눠 이를 해결합니다. <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crhhdohlWcx3"
      },
      "source": [
        "**Visualization of Multi-Head Attention**\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/vis_mh_attn.PNG\" height = 500>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RzVLLx_sdTR"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttn(nn.Module):\n",
        "  def __init__(self, emb_dim, n_head, device):\n",
        "    super().__init__()\n",
        "    self.W_Q, self.W_K, self.W_V = {}, {}, {}\n",
        "    self.n_head = n_head\n",
        "    self.device = device\n",
        "    for i in range(n_head):\n",
        "      self.W_Q[i] = nn.Linear(emb_dim, emb_dim // n_head, bias = False)\n",
        "      self.W_K[i] = nn.Linear(emb_dim, emb_dim // n_head, bias = False)\n",
        "      self.W_V[i] = nn.Linear(emb_dim, emb_dim // n_head, bias = False)\n",
        "\n",
        "    self.W_O = nn.Linear(emb_dim, emb_dim, bias = False)\n",
        "    self.attention = nn.ParameterDict()\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "    '''\n",
        "    inputs\n",
        "      - query(Tensor[batch_size, query_lenth, emb_dim)\n",
        "      - key(Tensor[batch_size, key_lenth, emb_dim])\n",
        "      - value(Tensor[batch_size, value_lenth, emb_dim])\n",
        "    returns:\n",
        "    '''\n",
        "    scale = torch.FloatTensor([emb_dim/self.n_head]).to(self.device) # scale = (d_k/h)\n",
        "    query, key, value = query.to(self.device), key.to(self.device), value.to(self.device)\n",
        "    # Q, K, V = (batch_size, seq_length, emb_dim)\n",
        "\n",
        "    for i in range(self.n_head):\n",
        "      Q, K, V = self.W_Q[i].to(self.device)(query), self.W_K[i].to(self.device)(key), self.W_V[i].to(self.device)(value)\n",
        "      # Q, K, V = (batch_size, seq_length, emb_dim // n_head)\n",
        "\n",
        "      similarity = torch.bmm(Q, K.transpose(1, 2)) / torch.sqrt(scale)\n",
        "      # similarity: (batch_size, query_length, key_length)\n",
        "\n",
        "      #masked attention\n",
        "      if mask is not None:\n",
        "        mask = mask.to(self.device)\n",
        "        similarity= similarity.masked_fill_(mask==0, -float('inf'))\n",
        "\n",
        "      similarity_p = F.softmax(similarity, dim = -1)\n",
        "      self.attention[str(i)] = torch.bmm(similarity_p, V)\n",
        "      # (batch_size, query_length, emb_dim // self.n_head)\n",
        "\n",
        "    multi_head_attn = torch.cat([self.attention[str(i)] for i in range(self.n_head)], dim = -1)\n",
        "    # (batch_size, query_length, emb_dim)\n",
        "\n",
        "    return self.W_O.to(self.device)(multi_head_attn)# (batch_size, query_length, emb_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsZDDPclCJk4"
      },
      "source": [
        "4. Vectorized code\n",
        "\n",
        "소프트웨어의 속도를 저하시키는 요인에는 여러가지가 있지만 <br>\n",
        "그 중 코드상에서 많이 나타나는 원인 중 하나는 과도한 반복문의 사용입니다. <br>\n",
        "위의 코드(Naive algorithm)는 Transformer의 장점인 빠른 속도를 상당히 저하시키기 때문에 실제로 사용되지는 않습니다. <br>\n",
        "\n",
        "벡터간 연산을 이용하면 반복문을 사용하지 않고 <br>\n",
        "Naive algorithm과 동일한 결과값을 얻을 수 있습니다. <br>\n",
        "\n",
        "Naive algorithm은 한 번에 하나씩 일을 처리하는 것과 같지만 <br>\n",
        "Vectorized code는 여러가지 일을 묶어서 한 번에 일괄적으로 처리하는 것과 같습니다. <br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/vectorized_code.PNG\" height = 300>\n",
        "</p>\n",
        "\n",
        "**Vectorized Multi-Head Attention**\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/vis_v_mh_attn.png\" height = 400>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTU1Qmx85f5Q"
      },
      "outputs": [],
      "source": [
        "class VectorizedMultiHeadAttn(nn.Module):\n",
        "  def __init__(self, emb_dim, n_head, device):\n",
        "    super().__init__()\n",
        "    self.W_Q = nn.Linear(emb_dim, emb_dim, bias = False)\n",
        "    self.W_K = nn.Linear(emb_dim, emb_dim, bias = False)\n",
        "    self.W_V = nn.Linear(emb_dim, emb_dim, bias = False)\n",
        "    self.W_O = nn.Linear(emb_dim, emb_dim, bias = False)\n",
        "    self.n_head = n_head\n",
        "    self.emb_dim = emb_dim\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "    emb_dim = self.emb_dim\n",
        "    batch_size, query_length, emb_dim = query.size()\n",
        "    key_length = key.size(1)\n",
        "    scale = torch.FloatTensor([emb_dim/self.n_head]).to(self.device)\n",
        "    query, key, value = query.to(self.device), key.to(self.device), value.to(self.device)\n",
        "\n",
        "    Q, K, V = self.W_Q(query), self.W_K(key), self.W_V(value)\n",
        "    # Q, K, V: (batch_size, seq_length, emb_dim)\n",
        "\n",
        "    Q = Q.contiguous().view(batch_size, query_length, self.n_head, emb_dim // self.n_head).transpose(1, 2)\n",
        "    K = K.contiguous().view(batch_size, key_length, self.n_head, emb_dim // self.n_head).transpose(1, 2)\n",
        "    V = V.contiguous().view(batch_size, key_length, self.n_head, emb_dim // self.n_head).transpose(1, 2)\n",
        "    # Q, K, V: (batch_size, self.n_head, seq_length, emb_dim//self.n_head)\n",
        "\n",
        "    Q = Q.contiguous().view(batch_size * self.n_head, query_length, emb_dim // self.n_head)\n",
        "    K = K.contiguous().view(batch_size * self.n_head, key_length, emb_dim // self.n_head)\n",
        "    V = V.contiguous().view(batch_size * self.n_head, key_length, emb_dim // self.n_head)\n",
        "    # Q. K, V: (batch_size * self.n_head, seq_length, emb_dim//n_head)\n",
        "\n",
        "    similarity = torch.bmm(Q, K.transpose(1, 2)) / torch.sqrt(scale)\n",
        "    # (batch_size * self.n_head, query_length, key_length)\n",
        "\n",
        "    # masked attention\n",
        "    if mask is not None:\n",
        "      mask = mask.to(self.device)\n",
        "      similarity= similarity.masked_fill_(mask==0, -float('inf'))\n",
        "\n",
        "    similarity_p = F.softmax(similarity, dim = -1)\n",
        "    # (batch_size * self.n_head, query_length, key_length)\n",
        "\n",
        "    multi_head_attn = torch.bmm(similarity_p, V)\n",
        "    # (batch_size * self.n_head, query_length, emb_dim // self.n_head)\n",
        "\n",
        "    multi_head_attn = multi_head_attn.contiguous().view(batch_size, self.n_head, query_length, emb_dim // self.n_head)\n",
        "    # (batch_size, n_head, query_length, emb_dim/self.n_head)\n",
        "\n",
        "    multi_head_attn = multi_head_attn.transpose(1, 2)\n",
        "    # (batch_size, query_length, self.n_head, emb_dim // self.n_head)\n",
        "\n",
        "    multi_head_attn = multi_head_attn.contiguous().view(batch_size, query_length, emb_dim)\n",
        "    # (batch_size, query_length, emb_dim)\n",
        "\n",
        "    return self.W_O(multi_head_attn) # (batch_size, query_length, emb_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToBa9ELQI4HF"
      },
      "source": [
        "## 결과값 비교(naive, vectorized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56eg5Sf9Jkhq",
        "outputId": "74a62489-6246-4055-8d5d-be284b0dfa40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative error between vectorized and navie: 0.0\n",
            "vectorized version is 4.78x faster than naive version (device: cuda, batch size: 1, seq_length: 3, emb_dim: 24, n_head: 8)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error (code from cs231n assignments)\"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "\n",
        "# init parameters\n",
        "batch_size = 1\n",
        "seq_length = 3\n",
        "emb_dim = 24\n",
        "n_head = 8\n",
        "x = torch.randn(batch_size, seq_length, emb_dim)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# multi head attn init\n",
        "multi_head_attn = MultiHeadAttn(emb_dim, n_head, device)\n",
        "vectorized_multi_head_attn = VectorizedMultiHeadAttn(emb_dim, n_head, device)\n",
        "\n",
        "# Linear layer 변수 matching\n",
        "vectorized_multi_head_attn.W_Q.weight = nn.Parameter(torch.cat([multi_head_attn.W_Q[i].weight for i in range(n_head)], dim = 0).to(device))\n",
        "vectorized_multi_head_attn.W_K.weight = nn.Parameter(torch.cat([multi_head_attn.W_K[i].weight for i in range(n_head)], dim = 0).to(device))\n",
        "vectorized_multi_head_attn.W_V.weight = nn.Parameter(torch.cat([multi_head_attn.W_V[i].weight for i in range(n_head)], dim = 0).to(device))\n",
        "vectorized_multi_head_attn.W_O.weight = nn.Parameter(multi_head_attn.W_O.weight.to(device))\n",
        "\n",
        "with torch.no_grad():\n",
        "  vectorized_multi_head_attn = vectorized_multi_head_attn.to(device)\n",
        "  vectorized_multi_head_attn.eval()\n",
        "  s = time.time()\n",
        "  vectorized_output = vectorized_multi_head_attn(query= x, key = x, value = x)\n",
        "  v_elapsed = time.time() - s\n",
        "\n",
        "  multi_head_attn = multi_head_attn.to(device)\n",
        "  multi_head_attn.eval()\n",
        "  s = time.time()\n",
        "  naive_output = multi_head_attn(query = x, key = x, value = x)\n",
        "  n_elapsed = time.time() - s\n",
        "\n",
        "  print('Relative error between vectorized and navie:', rel_error(vectorized_output.cpu().numpy(), naive_output.cpu().numpy()))\n",
        "  print('vectorized version is %.2fx faster than naive version (device: %s, batch size: %d, seq_length: %d, emb_dim: %d, n_head: %d)'\n",
        "   % ((n_elapsed / v_elapsed), device, batch_size, seq_length, emb_dim, n_head))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DX018QEKmtr"
      },
      "source": [
        "## Masked Multi-Head Attention\n",
        "\n",
        "Data를 모두 병렬처리함으로써 Transformer는 <br>\n",
        "정답을 모두 알려주고 학습을 진행하는 꼴로 바뀌게 됩니다. <br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/masked_attn_1.PNG\" height = 400>\n",
        "</p>\n",
        "\n",
        "**RNN Decoder**\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/translation_practice/main/pic/decoder.PNG\" height = 400>\n",
        "</p>\n",
        "\n",
        "\n",
        "우리는 TRansformer를 RNN의 recurrence 구조처럼 Encoder의 <br>\n",
        "\n",
        "> Context 벡터만을 이용하여 번역을 진행하는 경우 <br>\n",
        "> Context 벡터와 token[0]를 이용하여 번역을 진행하는 경우 <br>\n",
        "> context 벡터와 token[0], token[1]을 이용하여 번역을 진행하는 경우 <br>\n",
        "... <br>\n",
        "\n",
        "와 같이 이루고 싶습니다. <br>\n",
        "\n",
        "\n",
        "\n",
        "**Transformer Decoder**\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/masked_attn_2.PNG\" height = 400>\n",
        "</p>\n",
        "\n",
        "그러면 위의 그림과 같이 target sentence를 나누는 방식이 떠오르실 것입니다. <br>\n",
        "하지만 문제점은 해당 방법은 병렬처리를 할 수 없다는 점에 있습니다. <br>\n",
        "\n",
        "이를 해결하기 위해 input은 병렬로 feed 하되 Attention 연산을 아래와 같은 구조로 진행하여 유사도 확률을 0으로 만들면<br>\n",
        "0인 확률은 가중합의 결과에 영향을 끼치지 않으니 autoregressive 특성을 띠게됩니다. <br>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/masked_attn_3.PNG\" height = 300>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRp220xLEWuV",
        "outputId": "1475083d-8b0f-473c-d9b1-6cef20a897e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-18.6227,     -inf,     -inf,     -inf,     -inf],\n",
              "        [-35.2376, -11.0780,     -inf,     -inf,     -inf],\n",
              "        [ -3.2281, -23.5369, -46.5295,     -inf,     -inf],\n",
              "        [ 10.0105,  -2.7133,  44.1611,   6.4874,     -inf],\n",
              "        [-31.7343, -22.5114,  15.3287,   6.4891,  11.5723]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# init query, key\n",
        "batch_size, emb_dim = 1, 512\n",
        "query_length, key_length = 5, 5\n",
        "\n",
        "query = torch.randn(batch_size, query_length, emb_dim)\n",
        "key = torch.randn(batch_size, key_length, emb_dim)\n",
        "\n",
        "# similarity\n",
        "similarity = torch.bmm(query, key.transpose(1, 2)) # (batch_size, query_length, key_length)\n",
        "\n",
        "# masked attention!\n",
        "mask = torch.tril(torch.ones(query_length, key_length))\n",
        "masked_attended = similarity.masked_fill_(mask==0, -float('inf'))[0]\n",
        "masked_attended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV8d9cnCORzW",
        "outputId": "19e4ea4a-75ac-4f8c-a068-6681ddc58003"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "        [3.2183e-11, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "        [1.0000e+00, 1.5136e-09, 1.5648e-19, 0.0000e+00, 0.0000e+00],\n",
              "        [1.4744e-15, 4.3926e-21, 1.0000e+00, 4.3505e-17, 0.0000e+00],\n",
              "        [3.5540e-21, 3.5989e-17, 9.7703e-01, 1.4155e-04, 2.2832e-02]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.softmax(masked_attended, dim = -1) # similarity_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeRN6hpMjkKu"
      },
      "source": [
        "## Transformer(without Positional Encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GECzfkkZ7Rb"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, query_length, emb_dim, n_head, n_layer, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.embed = nn.Embedding(query_length, emb_dim)\n",
        "    #self.positional_encoding = PositionalEncoding()\n",
        "\n",
        "    self.encoder_layer = nn.ModuleList(\n",
        "        [EncoderBlock(emb_dim, n_head, device) for _ in range(n_layer)]\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    #transformer_input = self.positional_encoding(x) + self.embed(x)\n",
        "    transformer_input = self.embed(x.to(self.device))\n",
        "    for encoder_block in self.encoder_layer:\n",
        "      transformer_input =  encoder_block(transformer_input)\n",
        "\n",
        "    return transformer_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dtJVncOaAJo"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, emb_dim, n_head, device):\n",
        "    super().__init__()\n",
        "    self.n_head = n_head\n",
        "    self.device = device\n",
        "    self.multi_head_attn = VectorizedMultiHeadAttn(emb_dim, n_head, device)\n",
        "    self.norm1 = nn.LayerNorm(emb_dim)\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(emb_dim, 4*emb_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*emb_dim, emb_dim)\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.to(self.device)\n",
        "    attended = self.norm1(self.multi_head_attn(query = x, key = x, value = x) + x)\n",
        "    return self.norm2(self.feed_forward(attended) + attended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTB7PwyheHrr"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, query_length, emb_dim, n_head, n_layer, device, out_dim = 512):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.n_layer = n_layer\n",
        "    self.embed = nn.Embedding(query_length, emb_dim)\n",
        "    #self.positional_encoding = PositionalEncoding()\n",
        "\n",
        "    self.decoder_layer = nn.ModuleList(\n",
        "        [DecoderBlock(emb_dim, n_head, device) for _ in range(n_layer)]\n",
        "        )\n",
        "\n",
        "    self.fc_out = nn.Sequential(\n",
        "        nn.Linear(emb_dim, out_dim),\n",
        "        nn.Softmax(dim = -1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, enc_key, enc_value, tgt_mask):\n",
        "    #transformer_input = self.positional_encoding(x) + self.embed(x)\n",
        "    transformer_input = self.embed(x.to(self.device))\n",
        "    for decoder_block in self.decoder_layer:\n",
        "      transformer_input = decoder_block(transformer_input, enc_key, enc_value, tgt_mask)\n",
        "    out = transformer_input\n",
        "    return self.fc_out(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBL8sJwMeHHO"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, emb_dim, n_head, device):\n",
        "    super().__init__()\n",
        "    self.masked_multi_head_attn =VectorizedMultiHeadAttn(emb_dim, n_head, device)\n",
        "    self.norm1 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    self.multi_head_attn = VectorizedMultiHeadAttn(emb_dim, n_head, device)\n",
        "    self.norm2 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(emb_dim, 4*emb_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*emb_dim, emb_dim)\n",
        "    )\n",
        "    self.norm3 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "  def forward(self, x, enc_key, enc_value, tgt_mask):\n",
        "    query_length, key_length = x.size(1), enc_key.size(1)\n",
        "    x, enc_key, enc_value = x.to(device), enc_key.to(device), enc_value.to(device)\n",
        "\n",
        "    masked_attended= self.norm1(self.masked_multi_head_attn(x, x, x, tgt_mask) + x)\n",
        "    attended = self.norm2(self.multi_head_attn(masked_attended, enc_key, enc_value) + masked_attended)\n",
        "\n",
        "    return self.norm3(self.feed_forward(attended) + attended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuCt-Z30gQBy"
      },
      "outputs": [],
      "source": [
        "query_length = 10\n",
        "key_length= 10\n",
        "batch_size = 1\n",
        "n_tgt_tokens = 512\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = torch.randint(10, size = (batch_size, query_length))\n",
        "mask = torch.tril(torch.ones(query_length, key_length))\n",
        "\n",
        "encoder = TransformerEncoder(query_length = query_length, emb_dim = 512, n_head = 8, n_layer = 6, device = device)\n",
        "decoder = TransformerDecoder(query_length = query_length, emb_dim = 512, n_head = 8, n_layer = 6, device = device, out_dim = n_tgt_tokens)\n",
        "with torch.no_grad():\n",
        "  # naive Transformer Seq2Seq\n",
        "  encoder.eval()\n",
        "  encoder.to(device)\n",
        "  enc_out = encoder(x)\n",
        "\n",
        "  decoder.eval()\n",
        "  decoder.to(device)\n",
        "  dec_out = decoder(x, enc_out, enc_out, mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QloNJtqGibgs"
      },
      "source": [
        "## Positional Encoding(PE)\n",
        "Transformer의 경우 RNN과 다르게 recurrence 구조가 아니기 때문에 단어간의 위치에 대한 정보가 담기지 않습니다. <br>\n",
        "Transformer의 구조를 다시 한 번 상기해보면 <br>\n",
        "> (Masked) Multi-Head Attention <br>\n",
        "> Feed Forward network<br>\n",
        "> Layer Normalization + skip connection <br>\n",
        "> fc_out <br>\n",
        "\n",
        "로 이루어져 있습니다. <br>\n",
        "즉, Transformer는 sequential data를 통째로 받아 선형 변환(Linear Mapping)을 하는 구조이기 때문에 <br>\n",
        "위치에 대한 정보가 학습 중에 담기지 않습니다. <br>\n",
        "cf) RNN과 Transformer 비교\n",
        ">RNN: (token[i] -> RNN cell(Linear mapping) -> token[i+1]) <br>\n",
        ">Transformer: (sequence(tokens) -> Linear mapping -> sequence(tokens)) <br>\n",
        "\n",
        "추가로 Multi-Head Attention의 위치정보에 대해 생각 해보겠습니다. <br>\n",
        "\n",
        "$$Attention(i, j): \\sum 유사도(token[i],\\ token[j])value$$ <br>\n",
        "sequence_A = [i, am, your, father] <br>\n",
        "sequence_B = [am, i, your, father] <br>\n",
        "\n",
        "위의 두 문장을 각각 self attention 연산을 수행한다고 했을 때<br>\n",
        "i와 your사이의 Attention은 차이가 나지 않습니다. <br>\n",
        "즉, attention 연산은 순서와 무관한 집합으로서 결과값을 산출하게 됩니다. <br>\n",
        "하지만 문장의 뜻은 token의 순서에 따라 확연히 다른 의미를 가지게 됩니다. <br>\n",
        "\n",
        "\n",
        "이에 따라 Transformer가 sequential data를 다루기 위해서는 <br>\n",
        "추가적으로 위치에 대한 정보를 반영할 수 있는 encoding 방법이 필요합니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk-C24h_E3E-"
      },
      "source": [
        "## Positional Encoding?\n",
        "그러면 Positional Encoding은 무엇을 뜻하는 것인지 우선 알아보겠습니다. <br>\n",
        "\n",
        "sequence -> (Positional Encoding Mapping) -> 해당 sequence에서 각 token마다 가지는 고유한 번호(이를 통해 token의 위치정보를 알 수 있다.) <br>\n",
        "\n",
        "사실 Positional Encoding의 개념은 일상생활 속에서도 많이 사용되고 있습니다. <br>\n",
        "예를 들어 학교에서 학생들에게 번호를 부여한 후, 번호에 따라 어떤 행위(ex. 번호대로 줄 서세요)<br>\n",
        "를 부여할 때와 같은 경우가 Positional Encoding의 개념을 일상생활 속에서 적용한 경우입니다. <br>\n",
        "이를 프로그래밍적인 시야에서 바라보면 <br>\n",
        "list indexing에 Positional Encoding 개념이 적용되고 있다는 것을 알 수 있습니다. <br>\n",
        "```python\n",
        "sequence = ['i', 'am', 'your', 'father']\n",
        "sequence[0] = 'i'\n",
        "sequence[1] = 'am'\n",
        "...\n",
        "\n",
        "```\n",
        "\n",
        "## Ideal Criterial for Positional Encoding\n",
        "1. 각 위치의 Positional Encoding 값은 결정되어야(Deterministic)하는 동시에 유일해야 한다(Uniqueness).\n",
        "2. 데이터와 관계없이 각 위치의 Positional Encoding 값은 동일해야 한다.\n",
        "3. 확장성 (데이터의 길이와 관계없이 적용 가능해야 한다.)\n",
        "4. 간격이 동일하다면 위치와 관계없이 거리는 동일해야 한다.\n",
        "\n",
        "처음 위의 문장들을 접하시면 감이 잘 안오실 것이기 때문에 하나씩 자세히 설명드리겠습니다. <br>\n",
        "<br>\n",
        "1. 1-1) 각 위치의 Positional Encoding 값은 결정되어야 한다(Deterministic) <br>\n",
        "\n",
        "입력값이 정해지면 매번 같은 결과값을 산출해야합니다. <br>\n",
        "예를 들어, $y = x^2$에 $x = 2$를 대입하면 언제나 4라는 결과값이 산출됩니다.\n",
        "<br>\n",
        "\n",
        "1. 1-2) 각 위치의 Positional Encoding값은 유일해야 한다(Uniqueness)<br>\n",
        "\n",
        "sequence = [i, am, your, father] 일 때<br>\n",
        "sequence의 n번째 요소에 접근하기 위한 값은 유일해야 합니다. <br>\n",
        "즉 sequence[0] = 'i'로 결정되었으면 sequence[k] = 'i'를 만족하는 k가 존재해서는 안됩니다. <br>\n",
        "언뜻 보면 당연해보이는 이 문구는 python의 list indexing에서 지켜지지 않고 있는 것을 알 수 있습니다. <br>\n",
        "(물론 이는 편리성 제공을 목적으로 유일성 기준을 일부러 깬 것입니다.)<br>\n",
        "```python\n",
        "sequence = ['i', 'am', 'your', 'father']\n",
        "N = len(sequence) # N = 4\n",
        "sequence[N - 1] == sequence[-1] # True\n",
        "\n",
        "```\n",
        "cf) 유일성이 깨지는 흔한 경우는 주기를 가지는 경우입니다. <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "2. 데이터와 관계없이 각 위치의 Positional Encoding 값은 동일해야 한다. <br>\n",
        "\n",
        "sequence_1=[i, am, your, father] <br>\n",
        "sequence_2=[tick, tock] <br>\n",
        "\n",
        "모든 sequential data에 대해 n번째 요소에 접근하기 위한 Positional Encoding값은 동일해야 합니다. <br>\n",
        "즉, <br>\n",
        "sequence_1의 0번째 요소 -> sequence_1[0] <br>\n",
        "sequence_2의 0번째 요소 -> sequence_2[0] <br>\n",
        "... <br>\n",
        "모든 sequence의 0번째 요소 -> sequence[0] <br>\n",
        "<br>\n",
        "3. 확장성 (데이터의 길이와 관계없이 적용 가능해야 한다.) <br>\n",
        "\n",
        "길이가 가변적인 데이터에 대해 Positonal Encoding을 할 수 있어야 합니다. <br>\n",
        "극단적으로 예시를 들면 길이가 무한대인 경우에도 Positional Encoding을 통해 값을 할당할 수 있어야 합니다.\n",
        "<br>\n",
        "\n",
        "4. 간격이 동일하다면 위치와 관계없이 거리는 동일해야 한다.\n",
        "\n",
        "sequence = ['i', 'am', 'your', 'father'] 일 때, <br>\n",
        "distance('i', 'am') = distance('am', 'your') <br>\n",
        "이를 일반적인 수식으로 나타내면 다음과 같습니다.<br>\n",
        "$$distance(x, x+h) = distance(y, y+h)$$\n",
        "그러면 PE에 어떠한 기법들이 존재하는지 살펴보고 마지막으로 Transformer에 적용된 Sinusoidal Encoding을 이해해보도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZLeiQ8b_hZM"
      },
      "source": [
        "## 1. Count <br>\n",
        "sequence = [i, am, your, father] <br>\n",
        "\n",
        "해당 문장에 대한 위치정보를 순서대로 자연수를 이용하여 부여하는 방식입니다. <br>\n",
        "일반적인 python의 list indexing으로 생각할 수 있으며 다른 점은 음수를 통한 indexing은 배제한다는 점입니다. <br>\n",
        "```python\n",
        "#pseudo code\n",
        "sequence[i] = i번째 token (i: 0 or 자연수)\n",
        "```\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/count_encoding.PNG\" height = 300>\n",
        "</p>\n",
        "\n",
        "Count의 경우 Ideal Criterial를 모두 만족하지만 길이가 매우 긴 sequential data에 대해서는 지나치게 큰 PE의 값이 할당됩니다<br>\n",
        "문제점\n",
        "> 지나치게 큰 값을 모델에 지속적으로 feed할 시에 gradient exploding과 같은 문제점을 발생. <br>\n",
        "\n",
        "즉, 딥러닝 모델은 정규화된 PE 값을 feed할 필요성을 갖습니다. <br>\n",
        "cf) why? <br>\n",
        "-> Linear layer = Wx + b <br>\n",
        "-> Local gradient of W = x <br>\n",
        "-> 따라서 x의 크기가 클 경우 gradient exploding <br>\n",
        "for more detail, see(**multiply gate** of Patterns in backward flow): https://cs231n.github.io/optimization-2/#patterns\n",
        "\n",
        "\n",
        "## 2. Normalized Count\n",
        "가장 손쉬운 정규화 방법은 sequential data의 각 count를 n_token으로 나누는 방법입니다. <br>\n",
        "\n",
        "```python\n",
        "sequence = ['i', 'am', 'your', 'father']\n",
        "PE = [1/4, 2/4, 3/4, 4/4] # 4: len(sequence)\n",
        "```\n",
        "\n",
        "문제점<br>\n",
        "> 2번 조건을 위배합니다.<br>\n",
        "\n",
        "2. 데이터와 관계없이 각 위치의 Positional Encoding 값은 동일해야 한다. <br>\n",
        "\n",
        "```python\n",
        "sequence_1 = ['i', 'am', 'your', 'father']\n",
        "PE = [1/4, 2/4, 3/4, 4/4]\n",
        "\n",
        "sequence_2 = ['tick', 'tock']\n",
        "PE = [1/2, 2/2]\n",
        "```\n",
        "\n",
        "## 3. Binary Indexing\n",
        "\n",
        "2진수의 경우 0과 1로 이루어져있기 때문에 PE의 값이 지나치게 커질 우려가 없고 벡터로 표현하여 2번 조건을 위배하지 않을 것 같습니다. <br>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/binary_indexing.PNG\" height = 200>\n",
        "</p>\n",
        "\n",
        "문제점<br>\n",
        "> 3번 조건을 위배합니다. <br>\n",
        "> 4번 조건을 위배합니다.<br>\n",
        "\n",
        "3. 확장성 (데이터의 길이와 관계없이 적용 가능해야 한다.) <br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/binary_indexing_3.PNG\" height = 200>\n",
        "</p>\n",
        "\n",
        "위의 경우와 같이 sequential data를 이진수로 나타낼 때 기존 PE 벡터의 차원을 넘어가는 경우 새로 벡터를 형성해서 재할당 해줘야 합니다. <br>\n",
        "\n",
        "4. 간격이 동일하다면 위치와 관계없이 거리는 동일해야 한다. <br>\n",
        "\n",
        "```python\n",
        "#pseudo code\n",
        "PE(seq[1]) = [0, 1]\n",
        "PE(seq[2]) = [1, 0]\n",
        "PE(seq[3]) = [1, 1]\n",
        "\n",
        "distance([0, 1], [1, 0]) = 2**(1/2)\n",
        "distance([1, 0], [1, 1]) = 1\n",
        "\n",
        "```\n",
        "\n",
        "**solution idea** <br>\n",
        "\n",
        "3번 문제를 해결하기 위해서 벡터의 차원을 고정시키고 그 안에 실수를 우겨넣으면 해결됩니다. <br>\n",
        "이에 대한 근거로 실수는 특정 범위(ex 폐구간[0, 1])로 구간을 좁혀도 무한개의 실수를 가지고 있으므로 <br>\n",
        "1) normalize에 적합하고 <br>\n",
        "2) 데이터의 길이가 증가하여도 벡터의 차원이 한정된 채로 무한개의 실수가 배정되기 때문에 3번 조건(확장성)을 충족할 수 있습니다. <br>\n",
        "(Q: 어떻게 0<= x <= 1 일 때 x가 무한개 존재하나요? (x는 실수) A: ~~가까운 수학과에 물어보세요~~ 실수의 조밀성을 검색해주세요.) <br>\n",
        "\n",
        "cf) 해당 아이디어는 lookup table을 이용한 embedding 기법의 아이디어와도 유사한 부분이 있으므로 PE로 embedding을 사용할 수 있습니다. <br>\n",
        "(다만 이상적 PE 기준에 부합하게 학습이 될 것인지는 실험을 통해 확인을 해봐야겠지만 ~~뭐가 됐든 결과만 좋으면 끝~~) <br>\n",
        "-> embedding을 통한 PE에 더 자세히 알고 싶으시다면 Positional Embedding을 검색해주세요. <br>\n",
        "\n",
        "4번 문제의 경우를 해결하기 위해 위치에 따라 일정한 값으로 변환해주는 실수함수가 필요합니다. <br>\n",
        "$$즉, |f(pos + pos') - f(pos)| = distance(pos,\\ pos') $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB9SOMhQe11p"
      },
      "source": [
        "## Sinusoidal Positional Encoding\n",
        "solution idea를 통해 도출된 함수는 삼각함수(sin, cos)입니다. <br>\n",
        "(왜 갑자기 삼각함수냐고 물으신다면 저도 모릅니다. ~~수학과에 물어보세요 제발~~ ref: towardsdatascience blog 참고)\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/sinusoidal_positional_encoding.PNG\" height = 200>\n",
        "</p>\n",
        "\n",
        "$$pos(i, j) = sin(i/n^{2j/d}) $$\n",
        "$$(i: token's\\ position\\ in\\ sequence,\\ j: each\\ dimension,\\ n: some\\ real\\ number, \\ d: dimension) $$<br>\n",
        "sin함수를 통해 PE 값은 [-1, 1] 사이의 값을 갖게 됩니다. <br>\n",
        "위의 수식을 천천히 살펴봅시다. <br>\n",
        "sequence = ['i', 'am', 'your', 'father'] <br>\n",
        "PE(sequence[1]) -> 크기가 d인 행벡터, 벡터의 모든 값은 -1과 1사이의 실수 중 하나를 배정받는다.<br>\n",
        "i = 1인경우, 즉 sequence[1] = 'am'에 대해 다음과 같이 position encoding 됩니다.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/sinusoidal_positional_encoding_1.PNG\" height = 60>\n",
        "</p>\n",
        "\n",
        "<br>\n",
        "> 그래프를 통한 이해(일반적 버전)\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/sinusoidal_positional_encoding_2.PNG\" height = 350>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOVYv-lgV9xt"
      },
      "source": [
        "그러면 Sinusoidal PE가 이상적 기준을 모두 만족하는지 살펴보겠습니다. <br>\n",
        "1. 각 위치의 Positional Encoding 값은 결정되어야(Deterministic)하는 동시에 유일해야 한다(Uniqueness).\n",
        "\n",
        "-> 일반적으로 삼각함수는 주기(cycle)를 가지기 때문에 위의 조건을 만족하지 않습니다. <br>\n",
        "그러면 주기를 가지지 않는 함수로 값을 배정을 해야하는데 주기를 가지는 함수를 어떻게 주기를 가지지 않는 함수로 바꿀 수 있을까요? <br>\n",
        "\n",
        "A: 주기를 무한대로 만들면 됩니다. <br>\n",
        "\n",
        "먼저, sin함수의 경우 $(0,\\ {\\pi}/2)$ 에서 증가함수라는 것을 짚고 넘어가겠습니다. <br>\n",
        "\n",
        "**Suppose)** <br>\n",
        "PE들 중 마지막 열을 제외한 모든 값들이 동일한 경우가 있다고 가정을 하겠습니다. <br>\n",
        "즉, 마지막 열의 값만 서로 같다면 두 벡터는 동일하기 때문에 1번 조건에 위배됩니다. <br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/sinusoidal_positional_encoding_3.PNG\" height = 350>\n",
        "</p>\n",
        "\n",
        "n이 적당히 큰 수가 아닌 $\\lim_{n \\to \\infty} n$ 인 경우, <br>\n",
        "pos < pos' -> PE(pos) < PE(pos') 를 만족합니다. $(\\because \\ 증가함수)$ <br>\n",
        "즉 마지막 열의 값은 n이 충분히 클 경우 절대 동일한 값이 나오는 case가 존재하지 않습니다. <br>\n",
        "따라서 1번 조건을 만족합니다.\n",
        "\n",
        "2. 데이터와 관계없이 각 위치의 Positional Encoding 값은 동일해야 한다. <br>\n",
        "\n",
        "차원을 고정시키고 각 위치의 Positional Encoding의 값을 정적인 함수를 통해 실수값을 배정하므로 2번 조건을 만족합니다. <br>\n",
        "\n",
        "3. 확장성 (데이터의 길이와 관계없이 적용 가능해야 한다.)\n",
        "\n",
        "2번 조건과 마찬가지의 이유로 3번 조건을 만족합니다. <br>\n",
        "\n",
        "4. 간격이 동일하다면 위치와 관계없이 거리는 동일해야 한다.\n",
        "\n",
        "잠깐 곁길로 새서, 사실 여러분에게 아직 말하지 않은 사실이 있습니다. <br>\n",
        "논문에서 제시한 PE의 수식은 정확히 다음과 같습니다. <br>\n",
        "$$PE(i, 2j) = sin(i/n^{2j/d}) \\ \\ (짝수인 경우)$$\n",
        "$$PE(i, 2j+1) = cos(i/n^{2j/d}) \\ \\ (홀수인 경우)$$\n",
        "안그래도 골치아픈 수식이였는데 더 골치아파졌습니다. <br>\n",
        "\n",
        "$$PE = [sin(i/n^{0/d}),\\ cos(i/n^{0/d}),\\ sin(i/n^{2/d}),\\ cos(i/n^{2/d}),\\ ... ,\\ sin(i/n^{2}),\\ cos(i/n^{2})]$$\n",
        "\n",
        "sin함수를 쓸거면 sin함수만 쓰고 cos함수를 쓸거면 cos함수만 쓰지 도대체 왜 이럴까요? <br>\n",
        "-> Transformer는 선형 변환으로 이루어져 있기 때문에 <br>\n",
        "선형 변환을 통해 각 position data에 접근할 수 있는 것이 자연스럽고 좋은 결과를 도출할 것으로 기대됩니다. <br>\n",
        "$$ PE[pos] = Linear(PE[pos']) $$\n",
        "\n",
        "그러면 어떤 선형 변환을 통해 서로 전환이 될 수 있을까요? <br>\n",
        "<br>\n",
        "삼각함수는 결국 좌표공간과 벡터간의 이루는 각도를 통해 특정 좌표를 나타내는 방법입니다. <br>\n",
        "즉 삼각함수의 변수는 '각도' 입니다. <br>\n",
        "해당 아이디어를 통해 PE 벡터는 회전을 통해 다른 PE 벡터로 변환될 수 있다는 점이 떠오를 수 있습니다. <br>\n",
        "<br>\n",
        "**회전 변환**\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/rotation_transform.PNG\" height = 130>\n",
        "</p>\n",
        "\n",
        "간단한 예시를 위해 dimension = 1인 경우의 PE를 살펴보겠습니다.\n",
        "$$PE(\\theta) = [sin(\\theta/n^{0/1}),\\ cos(\\theta/n^{0/1})] = [sin(\\theta),\\ cos(\\theta)]$$\n",
        "$$PE(\\theta + \\phi) = [sin(\\theta + \\phi/n^{0/1}),\\ cos(\\theta + \\phi/n^{0/1})] = [sin(\\theta + \\phi),\\ cos(\\theta + \\phi)]$$\n",
        "\n",
        "위의 선형변환을 약간 변형하면 두 position간 변환이 이루어집니다. <br>\n",
        "그러면 n차원의 경우 어떻게 될까요? <br>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/rotation_transform_1.PNG\" height = 130>\n",
        "</p>\n",
        "\n",
        "이제 다시 원래의 명제로 돌아와서 <br>\n",
        "(4. 간격이 동일하다면 위치와 관계없이 거리는 동일한지) <br>\n",
        "에 대해 증명하겠습니다. <br>\n",
        "\n",
        "먼저, 인접한 Position간 각각의 dim(j)의 각도차이는 모두 동일한 것을 짚고 넘어가겠습니다. <br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/withAnewWorld/models_from_scratch/main/pic/rotation_transform_2.PNG\" height = 300>\n",
        "</p>\n",
        "\n",
        "여기서 $l$과 $l'$은 cos 제 2 공식에 의해 같습니다. <br>\n",
        "따라서 회전각도가 같으면 벡터간 거리는 같습니다. (n차원 증명은 여러분에게 맡기겠습니다. 화이팅)<br>\n",
        "그리고 sequential 데이터의 간격과 각도 차이는 위치와 관계없이 비례합니다. <br>\n",
        "따라서 4번 조건을 만족합니다.<br>\n",
        "\n",
        "\n",
        "cf) 논문에서는 PE벡터의 분모(n)로 10,000을 선택하였습니다. <br>\n",
        "왜 10,000인지에 대한 설명은 나오지 않지만 <br>\n",
        "1) floating error를 일으키지 않으면서 <br>\n",
        "2) 현재까지 다뤄지는 sequential data를 모두 다루기에 <br>\n",
        "충분히 큰 수라 선택하지 않았나 싶습니다. (실험적으로)\n",
        "\n",
        "$$PE(pos, 2i) = sin(pos/10,000^{2i/d})$$\n",
        "$$PE(pos, 2i+1) = cos(pos/10,000^{2i/d})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpviXb8CPMtJ"
      },
      "outputs": [],
      "source": [
        "# this code is from cs231n/assignments3/CS231n/transformer_layer.py (copy & paste)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes information about the positions of the tokens in the sequence. In\n",
        "    this case, the layer has no learnable parameters, since it is a simple\n",
        "    function of sines and cosines.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, dropout=0.1, max_len=5000):\n",
        "        \"\"\"\n",
        "        Construct the PositionalEncoding layer.\n",
        "        Inputs:\n",
        "         - embed_dim: the size of the embed dimension\n",
        "         - dropout: the dropout value\n",
        "         - max_len: the maximum possible length of the incoming sequence\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        #self.dropout = nn.Dropout(p=dropout)\n",
        "        assert embed_dim % 2 == 0\n",
        "        # Create an array with a \"batch dimension\" of 1 (which will broadcast\n",
        "        # across all examples in the batch).\n",
        "        pe = torch.zeros(1, max_len, embed_dim)\n",
        "\n",
        "        i=torch.arange(0, max_len).reshape(max_len, -1)\n",
        "        j= 10000**-(torch.arange(0, 1, 2/embed_dim))\n",
        "        pe[0, :, torch.arange(0, embed_dim, 2)]= torch.sin(i*j)\n",
        "        pe[0, :, torch.arange(1, embed_dim, 2)]= torch.cos(i*j)\n",
        "\n",
        "        # Make sure the positional encodings will be saved with the model\n",
        "        # parameters (mostly for completeness).\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Element-wise add positional embeddings to the input sequence.\n",
        "        Inputs:\n",
        "         - x: the sequence fed to the positional encoder model, of shape\n",
        "              (N, S, D), where N is the batch size, S is the sequence length and\n",
        "              D is embed dim\n",
        "        Returns:\n",
        "         - output: the input sequence + positional encodings, of shape (N, S, D)\n",
        "        \"\"\"\n",
        "        N, S, D = x.shape\n",
        "\n",
        "        output= x+self.pe[:, :S, :]\n",
        "        # output = self.dropout(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmJac0mbRpb7"
      },
      "source": [
        "## Heatmap of Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "E7k0C35cRwtf",
        "outputId": "636168ec-02fa-48b4-c1e9-ce238360f8aa"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# this code is from https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3 (copy & paste)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "dim = 128\n",
        "max_len = 256\n",
        "pe = PositionalEncoding(dim, max_len = max_len)\n",
        "x = torch.randn(1, max_len, dim)\n",
        "pe_matrix = pe(x) - x # this code is different from the original\n",
        "\n",
        "# heatmap plot\n",
        "plt.pcolormesh(pe_matrix[0], cmap='seismic') # I use different cmap for fun and to make difference vivid\n",
        "plt.xlabel('Dimension') # I think dimension is better than depth(from original) for unity\n",
        "plt.xlim((0, dim))\n",
        "plt.ylabel('Position')\n",
        "plt.title(\"PE matrix heat map\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBzEmIVeTI31"
      },
      "source": [
        "## Analysis of heatmap\n",
        "1) Dimension이 증가할수록 각 position간의 차이(세로선)는 미미합니다.(비슷한 색을 띰) <br>\n",
        "2) Dimension을 조금 더 줄여도 Deterministic & Uniqueness 조건을 만족할 것 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o28TtFPpRmvE"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUJHlz7zQgPi"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, query_length, emb_dim, n_head, n_layer, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.embed = nn.Embedding(query_length, emb_dim)\n",
        "    self.pe = PositionalEncoding(emb_dim)\n",
        "\n",
        "    self.encoder_layer = nn.ModuleList(\n",
        "        [EncoderBlock(emb_dim, n_head, device) for _ in range(n_layer)]\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    transformer_input = self.pe(self.embed(x.to(self.device)))\n",
        "    for encoder_block in self.encoder_layer:\n",
        "      transformer_input =  encoder_block(transformer_input)\n",
        "\n",
        "    return transformer_input\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, query_length, emb_dim, n_head, n_layer, device, out_dim = 512):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.n_layer = n_layer\n",
        "    self.embed = nn.Embedding(query_length, emb_dim)\n",
        "    self.pe = PositionalEncoding(emb_dim)\n",
        "\n",
        "    self.transformer_layer = nn.ModuleList(\n",
        "        [DecoderBlock(emb_dim, n_head, device) for _ in range(n_layer)]\n",
        "        )\n",
        "\n",
        "    self.fc_out = nn.Sequential(\n",
        "        nn.Linear(emb_dim, out_dim),\n",
        "        nn.Softmax(dim = -1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, enc_key, enc_value, tgt_mask):\n",
        "    transformer_input = self.pe(self.embed(x.to(self.device)))\n",
        "    for transformer_block in self.transformer_layer:\n",
        "      transformer_input = transformer_block(transformer_input, enc_key, enc_value, tgt_mask)\n",
        "    out = transformer_input\n",
        "    return self.fc_out(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PP-j5UGMQxVY"
      },
      "outputs": [],
      "source": [
        "query_length = 10\n",
        "key_length= 10\n",
        "batch_size = 1\n",
        "n_tgt_tokens = 512\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = torch.randint(10, size = (batch_size, query_length))\n",
        "mask = torch.tril(torch.ones(query_length, key_length))\n",
        "\n",
        "encoder = TransformerEncoder(query_length = query_length, emb_dim = 512, n_head = 8, n_layer = 6, device = device)\n",
        "decoder = TransformerDecoder(query_length = query_length, emb_dim = 512, n_head = 8, n_layer = 6, device = device, out_dim = n_tgt_tokens)\n",
        "with torch.no_grad():\n",
        "  # naive Transformer Seq2Seq\n",
        "  encoder.eval()\n",
        "  encoder.to(device)\n",
        "  enc_out = encoder(x)\n",
        "\n",
        "  decoder.eval()\n",
        "  decoder.to(device)\n",
        "  dec_out = decoder(x, enc_out, enc_out, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKIzV9MpRHlQ",
        "outputId": "8242e0fa-4e35-47d0-b099-53a729b0afad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 512])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dec_out.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdQppmt1urFT"
      },
      "source": [
        "## Transformer의 장점\n",
        "그래서 RNN으로 줄곧 잘 해왔는데 Transformer를 개발한 이유가 뭘까요? <br>\n",
        "많은 이유가 있지만 크게 다음의 두가지로 압축할 수 있을 것 같습니다. <br>\n",
        "\n",
        "1. 성능\n",
        "2. 속도\n",
        "\n",
        "1) 성능\n",
        "\n",
        ">Transformer 이전 번역 문제에서 SOTA 모델은 Attention 기반 RNN Seq2Seq 였습니다. <br>\n",
        ">RNN 모델은 Multi-Head Attention이 아닌 단일 Attention을 수행하여 성능이 Transfomer 만큼 뛰어나지 않습니다. <br>\n",
        ">즉 Multi-Head Attention 모듈이 Transformer 성능의 핵심입니다. <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "2) 속도\n",
        "> Recurrence구조를 가지는 RNN과 다르게 Transformer는 병렬처리를 통해 데이터를 모두 처리합니다. <br>\n",
        "> 병렬처리의 위력은 Vectorized code에서 잘 보셨을 것입니다. <br>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "의의\n",
        "> 일반적으로 모델의 성능과 속도는 trade-off 관계에 있습니다. <br>\n",
        "> 하지만 Transformer는 이러한 반비례 관계를 깨버린 동시에 <br>\n",
        "> 통역사와 비슷한 수준의 번역 성능을 보여 NLP 영역에 판도를 뒤바꾸었습니다. <br>\n",
        "> 이제 Transformer 모델은 NLP에 국한되는 것이 아닌 Deep Learning 전반에 깊게 뿌리 뻗었습니다. <br>\n",
        "> 심지어 RNN뿐 아니라 CNN 또한 Transformer로 대체하는 분위기가 형성되었고 결과 또한 CNN을 능가하고 있습니다. <br>\n",
        "> ex) ViT, DETR, DINO(Transformer + self supervised learning), ... <br>\n",
        "> 해당 notebook을 통해 Transformer의 모든 것을 이해하셨길 바랍니다 :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4wfRQMMVA-6"
      },
      "source": [
        "## Try!\n",
        "1. code Refactoring <br>\n",
        "최대한 깔끔한 코드로 나타내려했지만 역량 한계 & 설명의 필요성 때문에 코드가 지저분합니다. <br>\n",
        "조금 더 유지보수하기 쉽고 깔끔한, 그리고 error에 대응하는 코드로 Refactoring 해보세요! <br>\n",
        "Plus) 조금 더 속도가 빠르고(more vectorized) 메모리 사용이 적은 모델로 Refactoring 해보세요! <br>\n",
        "\n",
        "2. Make your own Transformer <br>\n",
        "이해한 내용을 바탕으로 논문과 reference, ref code 등을 이용하여 여러분이 직접 Transformer를 구현해보세요!\n",
        "\n",
        "3. NLP task using Transformer <br>\n",
        "Transformer를 이용하여 NLP task(번역, 텍스트 분류 등)를 수행해보세요! <br>\n",
        "cf) Transformer를 통해 Vision task(image classification, object dection, ...)에도 도전해보세요!<br>\n",
        "\n",
        "4. Design model based on Transformer <br>\n",
        "현재 다양한 NLP영역에서 SOTA성능을 보이는 GPT, BERT와 같은 Transformer 기반 model을 설계해보세요!\n",
        "\n",
        "5. **설계 / 설명 오류, 생략된 이론 등을 말해주세요!** <br>\n",
        "같이 성장해요 :)\n",
        "\n",
        "6. anything with your creativity! <br>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QloNJtqGibgs"
      ],
      "name": "Transformer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "811efbe449f8030a1357d21838ca884c523c3e7a6d4d3bfbec51bb5a60890126"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}